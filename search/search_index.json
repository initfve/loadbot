{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"lbot - workload database driver (currently supporting only MongoDB)","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The purpose of this tool is to simulate workloads to facilitate testing the failover capabilities of database cluster under load. This code, being an open-source project, is in its early development stage and likely contains various bugs.</p>"},{"location":"#how-to-use","title":"How to use:","text":"<ol> <li>Build image - <code>make build-docker</code></li> <li>Run agent - <code>make run-docker-agent</code></li> <li>Configure agent by running lbot config command - <code>make run-docker-lbot-config CONFIG_FILE=\"_config.json\"</code></li> <li>Run workload tests - <code>make run-docker-lbot start</code></li> </ol> <p>Note: If running with local db remember to use host network and configure connection_string to 127.0.0.1 <code>docker run --network=\"host\" --rm -t mload &lt; config_file.json</code> or check Makefile</p> <p>This tool offers two ways to access it: one through CLI arguments and the other via a configuration file. Utilizing the configuration file provides additional functionalities for the tool.</p>"},{"location":"#cli-usage","title":"CLI usage:","text":"<pre><code>A command-line database workload\n\nUsage:\n  lbot [command]\n\nDriver Commands:\n  config      Config\n  start       Start stress test\n  stop        Stop stress test\n\nAdditional Commands:\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n\nFlags:\n  -u, --agent-uri string    loadbot agent uri (default: 127.0.0.1:1234)\n  -h, --help                help for lbot\n  -v, --version             version for lbot\n\nUse \"lbot [command] --help\" for more information about a command.\n</code></pre> <p>Known issue: * srv not working with some DNS servers - golng 1.13+ issue see this and this</p> <pre><code>&gt; Old versions of kube-dns and the native DNS resolver (systemd-resolver) on Ubuntu 18.04 are known to be non-compliant in this manner.\n</code></pre>"},{"location":"USAGE/","title":"USAGE","text":""},{"location":"USAGE/#configuration-file","title":"Configuration file:","text":"<p>Due to the limited functionalities of the CLI, in order to fully harness the capabilities of this tool, it is advisable to utilize a configuration file. The program can be executed by specifying the configuration file with: 1. stdin with binary <code>cat config_file.json | mload</code> or stdin with docker <code>docker run mload -t mload &lt; config_file.json</code> 2. flag with <code>--config-file &lt;file-path&gt;</code> or <code>-f &lt;file-path&gt;</code>. </p> <p>Note: If you want to use config file with docker you need to mount volume with file or copy when building image.</p> <p>Example file:</p> <pre><code>{\n  \"connection_string\": \"mongodb://localhost:27017\",\n  \"debug\": true,\n  \"jobs\": [\n    {\n      \"name\": \"Write 100c 1k ops\",\n      \"type\": \"write\",\n      \"schema\": \"user_schema\",\n      \"connections\": 100,\n      \"operations\": 1000,\n    },\n    {\n      \"name\": \"Dummy job name/ read 30s 100rps\",\n      \"type\": \"read\",\n      \"schema\": \"user_schema\",\n      \"connections\": 100,\n      \"pace\": 100,\n      \"duration\": \"30s\",\n      \"filter\": {\n          \"special_name\": \"#special_name\"\n      }\n    }\n  ],\n  \"schemas\": [\n    {\n      \"name\": \"user_schema\",\n      \"database\": \"load_test\",\n      \"collection\": \"load_test\",\n      \"schema\": {\n        \"_id\": \"#_id\",\n        \"special_name\": \"#string\",\n        \"lastname\": \"#string\"\n      },\n      \"save\": [\n        \"special_name\"  // will be avaliable in job.filter under \"#special_name\"\n      ]\n    },\n  ],\n  \"reporting_formats\": [\n    {\n      \"name\": \"simple\",\n      \"interval\": \"5s\",\n      \"template\": \"Job: {{.JobType}}, total reqs: {{.TotalReqs}}, RPS {{f2 .Rps}} success: {{.SuccessReqs}}\\n\\n\"\n    }\n  ]\n}\n</code></pre> Defining schemas   **Schema fields**  - `name` - unique name, used in jobs (see job.schema) for determining which template use - `database` - database name - `collection` - collection name - `schema` - actual document template  **Schema document template fields:**  General - `#id`  - `#string` - `#word`  Internet - `#email` - `#username` - `#password`  Person - `#name` - `#first_name` - `#first_name_male` - `#first_name_female` - `#last_name` - `#title_male` - `#title_female` - `#phone_number`  **More examples**   Defining Jobs   **Example write with schema 100ops**  <pre><code>{\n  \"name\": \"insert with schema\",\n  \"type\": \"write\",\n  \"schema\": \"user_schema\",\n  \"connections\": 10,\n  \"operations\": 100\n}\n</code></pre>  **Write without schema 20s**  <pre><code>{\n  \"name\": \"insert without schema\",\n  \"type\": \"write\",\n  \"database\": \"load_test\",\n  \"collection\": \"load_test\",\n  \"connections\": 10,\n  \"data_size\": 100,\n  \"duration\": \"20s\",\n  \"timeout\": \"1s\"\n}\n</code></pre>  **Read with schema 20s**  <pre><code>{\n  \"name\": \"read with schema\",\n  \"type\": \"read\",\n  \"schema\": \"user_schema\",\n  \"connections\": 10,\n  \"operations\": 100,\n  \"filter\": {\n    \"user_name\": \"#user_name\",\n    \"name\": \"#generate_value\"  // here you can use remember/saved value as well as generated one\n  }\n}\n</code></pre>  **Let the database rest**  <pre><code>{\n  \"type\": \"sleep\",\n  \"duration\": \"5s\"\n}\n</code></pre>  **Drop collection**  <pre><code>{\n  \"type\": \"drop_collection\",\n  \"database\": \"load_test\",\n  \"collection\": \"load_test\",\n  \"operations\": 1\n}\n</code></pre> or with schema <pre><code>{\n  \"type\": \"drop_collection\",\n  \"schema\": \"example_schema\",\n  \"operations\": 1\n}\n</code></pre>  **Jobs fields:**  * `name`(string, optional) - job name * `type`(enum `write|bulk_write|read|update|create_index|drop_collection|sleep`) - operation type * `template`(string) - schema name, if you will not provide schema data will be inserted in `{'data': }` format * `database`(string, required if schema is not set) - database name * `schema`(string, optional) - string foreign-key to schemas list * `filter`(string, required for read and update) - filter schema * `indexes`(list, optional) - list of indexes to create (only for type \"create_index\")  * `format`(string, optional) - string foreign-key to reporting_formats list * `collection`(string, required if schema is not set) - collection name * `connection`(unsigned int) - number of concurrent connections, number is not limited to physical threads number * `data_size`(unsigned int) - data size inserted (currently only works for default schema) * `batch_size`(unsigned int) - insert batch size (only applicable for `bulk_write` job type) * `duration`(string) - duration time ex. 1h, 15m, 10s * `operations`(unsigned int) - number of requests to perform, ex. 100 reads, 100 bulk_writes * `timeout`(string) - connection timeout ex. 1h, 15m, 10s   Custom reporting format   By default, you have access to several available output formats: `default`, `simple`, `write`, and `bulk_write`. If you do not provide the 'default' format, the default format associated with the type will be utilized. Moreover, you have the flexibility to override all formats except for the `default` one.  **default**  <pre><code>2023/08/12 15:10:44 Job: \"lovely job name\"\nReqs: 7500, RPS 500.00, s:7500/err:0/tout:0/errRate:0.0%\nAVG: 1.395ms P50: 0.484ms, P90: 0.703ms P99: 8.048ms\n</code></pre>  **simple**  <pre><code>2023/08/12 17:28:47 Reqs: 2592, RPS 500.05 s:2592/err:0\n</code></pre>  **write**  <pre><code>2023/08/12 17:28:52 Reqs: 2595, RPS 499.91, s:2595/err:0/tout:0/errRate:0.0%\nAVG: 1.987ms P50: 0.989ms, P90: 1.634ms P99: 34.556ms\n</code></pre>  **bulk_write**  <pre><code>2023/08/12 17:28:57 Reqs: 2590, OPS: 259000, RPS 499.50, OPS 49949.52, s:2590/err0/tout:0/errRate:0.0\nAVG: 6.867ms P50: 3.814ms, P90: 6.786ms P99: 79.938ms\n</code></pre>  **Example custom reporting format**  <pre><code>{\n  \"name\": \"custom\",\n  \"interval\": \"5s\",\n  \"template\": \"{{.Now}} Job: {{.JobType}}, total reqs: {{.TotalReqs}}, RPS {{f2 .Rps}} success: {{.SuccessReqs}}\\n\\n\"\n}\n</code></pre> - `name` - used to determine which template to use (see section job.format) - `interval` - if set, tests reports/summaries will be displayed at set time intervals - `template` - report format   **Template fields**  `Now`, `JobName`, `JobType`, `JobBatchSize`,`SuccessReqs`, `ErrorReqs`, `TotalReqs`, `TotalOps`, `TimeoutErr`, `NoDataErr`, `OtherErr`, `ErrorRate`, `Rps`, `Ops`  **Math fields**  `Min`, `Max`, `Avg`, `Rps` and `P` ex. `P90` - percentiles  **Floating point fields formatters**  `f` - format number to n places (1 to 4) ex. `{{f2 .Rps}}`   `msf` - format number to n places (1 to 4) and convert to milliseconds ex. `{{msf2 .P99}}`     More examples   - Index creation job <pre><code>{\n  \"type\": \"create_index\",\n  \"template\": \"default\",\n  \"indexes\": [\n    {\n      \"keys\": { \"name\": 1 },\n      \"options\": { \"unique\": false, \"name\": \"dummy_name_index_name\" },\n    }\n  ]\n}\n</code></pre> or without using schema <pre><code>{\n  \"type\": \"create_index\",\n  \"database\": \"load_test\",\n  \"collection\": \"load_test\",\n  \"operations\": 1,\n  \"indexes\": [\n    {\n      \"keys\": {\"name\": 1},\n    }\n  ]\n}\n</code></pre> Other features   **Features**  - JSON standardization - comments and trailing commas support ex. <pre><code>{\n    \"jobs\": [\n        {\n          \"type\": \"drop_collection\",\n          \"database\": \"load_test\",\n          \"collection\": \"load_test\",\n          \"operations\": 1\n        },\n        /*{\n          \"type\": \"sleep\",\n          \"duration\": \"5s\",\n          \"format\": \"simple\"\n        },*/\n    ]\n}\n</code></pre> <p>Note: If you don't provide the requests amount or duration limit program will continue running  indefinitely unless it is manually stopped by pressing <code>ctrl-c</code>. </p>"},{"location":"getting_started/install/","title":"Installation","text":""},{"location":"getting_started/install/#using-homebrew-macoslinux","title":"Using Homebrew (MacOS/Linux)","text":"<p>If you have homebrew, the installation is as simple as: <pre><code>brew tap kuzxnia/loadbot\nbrew install lbot\n</code></pre></p>"},{"location":"getting_started/install/#by-downloading-the-binaries-macoslinux","title":"By downloading the binaries (MacOS/Linux)","text":"<ol> <li>Go to the releases and download    the latest release archive for your platform.</li> <li>Extract the archive.</li> <li>Move the binary to somewhere in your <code>PATH</code>.</li> </ol> <p>Sample steps for MacOS: <pre><code>$ VERSION=&lt;VERSION_TAG&gt;\n$ wget https://github.com/utkuozdemir/pv-migrate/releases/download/${VERSION}/pv-migrate_${VERSION}_darwin_x86_64.tar.gz\n$ tar -xvzf pv-migrate_${VERSION}_darwin_x86_64.tar.gz\n$ mv pv-migrate /usr/local/bin\n$ pv-migrate --help\n</code></pre></p>"},{"location":"getting_started/install/#running-directly-in-docker-container","title":"Running directly in Docker container","text":"<p>Alternatively, you can use the official Docker images that come with the <code>pv-migrate</code> binary pre-installed: <pre><code>docker run --rm -it kuzxnia/lbot:&lt;IMAGE_TAG&gt; .\ndocker run --rm -it kuzxnia/lbot-agent:&lt;IMAGE_TAG&gt; .\n</code></pre></p>"},{"location":"getting_started/install/#_1","title":"Install Guide","text":"<p>Using Homebrew (MacOS/Linux) If you have homebrew, the installation is as simple as:</p> <p>brew tap utkuozdemir/pv-migrate brew install pv-migrate</p> <ul> <li> <p>install brew /tap brew tap kuzxnia/loadbot brew install lbot</p> </li> <li> <p>build from sources</p> </li> <li> <p>build with docker</p> </li> </ul>"}]}